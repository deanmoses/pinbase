"""Scrape images for pinball machines that don't have artwork.

Quick-and-dirty tool for demo purposes — NOT for production use.

Tries four strategies in order:
0. Hand-picked URLs (MANUAL_IMAGES dict, keyed by opdb_id)
1. Copy from a group sibling (same franchise, different edition)
2. Scrape IPDB page (for machines with IPDB IDs)
3. Search Bing Images (fallback for everything else)

Usage:
    python manage.py scrape_images                    # all machines
    python manage.py scrape_images --year-min 2024    # only 2024+
    python manage.py scrape_images --dry-run           # preview without saving
"""

from __future__ import annotations

import logging
import re
import time
from html import unescape
from urllib.parse import quote_plus, urljoin

import requests
from bs4 import BeautifulSoup
from django.core.management.base import BaseCommand

from apps.machines.models import Claim, MachineModel, Source
from apps.machines.resolve import resolve_model

logger = logging.getLogger(__name__)

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
}

# Seconds to wait between network requests (be polite).
REQUEST_DELAY = 1.5

# Hand-picked image URLs for machines that scrapers can't find. Keyed by opdb_id.
MANUAL_IMAGES: dict[str, list[str]] = {
    "GV8wB-Mq12N": [  # Pokémon (Pro)
        "https://scontent-sjc6-1.xx.fbcdn.net/v/t39.30808-6/635610770_1323098703181952_4251437457470395789_n.jpg?stp=cp6_dst-jpg_tt6&_nc_cat=107&ccb=1-7&_nc_sid=7b2446&_nc_ohc=3ktPut9lrCcQ7kNvwFY0hBU&_nc_oc=Adn38G1p2Sc0iczV8POYOI55e2jFFUFvgL0HKF5JI2Cg0EhKPVjZaQuTzkJyV7eaV-k&_nc_zt=23&_nc_ht=scontent-sjc6-1.xx&_nc_gid=Fj6p4Hief3yQdoxOcCW5kw&oh=00_AfuFs3vRocrxVq9YIBjrM6M75viyrasY2yKTujoSMOAuQQ&oe=699D84F1",
    ],
    "GV8wB-MRjKd": [  # Pokémon (Premium/LE)
        "https://image-cdn.hypb.st/https%3A%2F%2Fhypebeast.com%2Fimage%2F2026%2F02%2F16%2Fstern-pinball-pokemon-pinball-machine-debut-collaboration-release-info-001.jpg?w=1440&cbr=1&q=90&fit=max",
    ],
    "GV8wB-MRjKd-AOVy7": [  # Pokémon (Premium)
        "https://image-cdn.hypb.st/https%3A%2F%2Fhypebeast.com%2Fimage%2F2026%2F02%2F16%2Fstern-pinball-pokemon-pinball-machine-debut-collaboration-release-info-001.jpg?w=1440&cbr=1&q=90&fit=max",
    ],
}


def _has_images(extra_data: dict) -> bool:
    """Check if extra_data already contains usable image URLs."""
    if extra_data.get("image_urls"):
        return True
    images = extra_data.get("images")
    if images and isinstance(images, list):
        return True
    return False


def _try_group_sibling(pm: MachineModel) -> list[str] | None:
    """Copy image URLs from a sibling in the same group."""
    if not pm.group:
        return None
    for sib in pm.group.machine_models.exclude(pk=pm.pk):
        ed = sib.extra_data or {}

        # Try OPDB structured images — extract best URLs.
        images = ed.get("images")
        if images and isinstance(images, list):
            urls = []
            for img in images:
                if not isinstance(img, dict):
                    continue
                img_urls = img.get("urls", {})
                url = (
                    img_urls.get("large")
                    or img_urls.get("medium")
                    or img_urls.get("small")
                )
                if url:
                    urls.append(url)
            if urls:
                return urls

        # Try IPDB flat URL list.
        ipdb_urls = ed.get("image_urls")
        if ipdb_urls and isinstance(ipdb_urls, list):
            return list(ipdb_urls)

    return None


def _try_ipdb_scrape(ipdb_id: int) -> list[str] | None:
    """Scrape image URLs from an IPDB machine page."""
    url = f"https://www.ipdb.org/machine.cgi?id={ipdb_id}"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.warning("IPDB request failed for id=%s: %s", ipdb_id, e)
        return None

    soup = BeautifulSoup(resp.text, "html.parser")
    urls = []

    # IPDB thumbnails: /images/{id}/tn_image-{n}.png
    for img in soup.find_all("img"):
        src = img.get("src", "")
        if f"/images/{ipdb_id}/" in src:
            full_url = urljoin("https://www.ipdb.org/", src)
            urls.append(full_url)

    return urls if urls else None


def _try_bing_images(query: str) -> list[str] | None:
    """Search Bing Images and return the first few result URLs."""
    search_url = f"https://www.bing.com/images/search?q={quote_plus(query)}&first=1"
    try:
        resp = requests.get(search_url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
    except requests.RequestException as e:
        logger.warning("Bing search failed for %r: %s", query, e)
        return None

    # Bing HTML-encodes quotes in data attributes; decode before parsing.
    text = unescape(resp.text)

    # Extract original image URLs from 'murl' fields in Bing's JSON metadata.
    urls = re.findall(r'"murl":"(https?://[^"]+)"', text)

    # Filter for actual image file URLs.
    image_exts = (".jpg", ".jpeg", ".png", ".webp")
    good_urls = [u for u in urls if any(u.lower().endswith(ext) for ext in image_exts)]

    # If strict extension filtering is too aggressive, fall back to all murl hits.
    if not good_urls:
        good_urls = [u for u in urls if "." in u.split("/")[-1]]

    return good_urls[:3] if good_urls else None


class Command(BaseCommand):
    help = "Scrape images for machines without artwork (demo tool)."

    def add_arguments(self, parser):
        parser.add_argument(
            "--year-min",
            type=int,
            default=None,
            help="Only process machines from this year onward.",
        )
        parser.add_argument(
            "--dry-run",
            action="store_true",
            help="Preview what would be scraped without saving.",
        )

    def handle(self, *args, **options):
        year_min = options["year_min"]
        dry_run = options["dry_run"]

        source, _ = Source.objects.update_or_create(
            slug="web-scrape",
            defaults={
                "name": "Web Scrape",
                "source_type": "other",
                "priority": 10,
                "url": "",
                "description": "Temporary web-scraped images for demo purposes.",
            },
        )

        # Find machines without images, newest first.
        qs = MachineModel.objects.filter(alias_of__isnull=True).order_by(
            "-year", "name"
        )
        if year_min:
            qs = qs.filter(year__gte=year_min)

        machines = [pm for pm in qs if not _has_images(pm.extra_data or {})]

        total = len(machines)
        if total == 0:
            self.stdout.write(self.style.SUCCESS("All machines already have images!"))
            return

        self.stdout.write(f"Found {total} machines without images.\n")

        if dry_run:
            self.stdout.write(self.style.WARNING("DRY RUN — nothing will be saved.\n"))

        found_count = 0
        for i, pm in enumerate(machines, 1):
            strategy, urls = self._find_images(pm)

            if urls:
                found_count += 1
                if not dry_run:
                    Claim.objects.assert_claim(
                        model=pm,
                        source=source,
                        field_name="image_urls",
                        value=urls,
                    )
                    resolve_model(pm)
                self.stdout.write(
                    self.style.SUCCESS(
                        f"  [{i}/{total}] \u2713 {pm.name} [{pm.year}] \u2014 {strategy} ({len(urls)} URLs)"
                    )
                )
            else:
                self.stdout.write(
                    self.style.WARNING(
                        f"  [{i}/{total}] \u2717 {pm.name} [{pm.year}] \u2014 no images found"
                    )
                )

        if not dry_run:
            from apps.machines.cache import invalidate_all

            invalidate_all()

        action = "would be updated" if dry_run else "updated"
        self.stdout.write(
            self.style.SUCCESS(f"\nDone! {found_count}/{total} machines {action}.")
        )

    def _find_images(self, pm: MachineModel) -> tuple[str | None, list[str] | None]:
        """Try each strategy in order, return (strategy_name, urls) or (None, None)."""

        # Strategy 0: Hand-picked URLs (no network needed).
        if pm.opdb_id and pm.opdb_id in MANUAL_IMAGES:
            return "manual", MANUAL_IMAGES[pm.opdb_id]

        # Strategy 1: Group sibling (no network needed).
        urls = _try_group_sibling(pm)
        if urls:
            return "group sibling", urls

        # Strategy 2: IPDB scrape.
        if pm.ipdb_id:
            time.sleep(REQUEST_DELAY)
            urls = _try_ipdb_scrape(pm.ipdb_id)
            if urls:
                return "IPDB", urls

        # Strategy 3: Bing Images.
        time.sleep(REQUEST_DELAY)
        year_part = f" {pm.year}" if pm.year else ""
        query = f'"{pm.name}" pinball{year_part}'
        urls = _try_bing_images(query)
        if urls:
            return "Bing Images", urls

        return None, None
